{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imblearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (\n",
    "    decomposition,\n",
    "    discriminant_analysis,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    naive_bayes,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"public_data\")\n",
    "\n",
    "DROP_VARS = [\"ADMITTIME\", \"DISCHTIME\", \"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "features = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_feat.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_label.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "x_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_train.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_df.drop(columns=DROP_VARS, inplace=True)\n",
    "\n",
    "ys = pd.Series(\n",
    "    pd.read_csv(\n",
    "        DATA / \"mimic_synthetic_train.solution\",\n",
    "        header=None,\n",
    "        names=labels,\n",
    "        sep=\" \",\n",
    "    ).values.flatten()\n",
    ")\n",
    "\n",
    "# Load testing set\n",
    "x_test_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_test.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_test_df.drop(columns=DROP_VARS, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prev (train, test):\n",
    "    \"\"\"\n",
    "    Filling the cell containing NaN values with previous entry\n",
    "    \"\"\"\n",
    "    \n",
    "    na_cols = set(train.columns[train.isna().any()])\n",
    "    for col in na_cols:\n",
    "        train[col] = train[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    na_cols =  set(test.columns[test.isna().any()])\n",
    "    for col in na_cols:\n",
    "        test[col] = test[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, x_test_df = fill_prev(x_df, x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rare_categories(dfs, col, keeps=None, keep_n=5):\n",
    "    if keeps is None:\n",
    "        keeps = x_df[col].value_counts()[:keep_n].index\n",
    "        print(keeps)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.loc[~df[col].isin(keeps), col] = \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CATHOLIC', 'NOT_SPECIFIED', 'UNOBTAINABLE', 'PROTESTANT_QUAKER',\n",
      "       'JEWISH'],\n",
      "      dtype='object')\n",
      "Index(['ENGL', 'SPAN'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "merge_rare_categories([x_df, x_test_df], col=\"RELIGION\", keep_n=5)\n",
    "merge_rare_categories([x_df, x_test_df], col=\"LANGUAGE\", keep_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Hot enconding\n",
    "x_all_1hot_df = pd.get_dummies(pd.concat([x_df, x_test_df]))\n",
    "\n",
    "x_1hot_df = x_all_1hot_df[: len(x_df)]\n",
    "x_test_1hot_df = x_all_1hot_df[len(x_df) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobo/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "const_cols = {col for col in x_1hot_df if len(x_1hot_df[col].unique()) == 1}\n",
    "x_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n",
    "x_test_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_1hot_df = scaler.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df =scaler.fit_transform(x_test_1hot_df)\n",
    "\n",
    "#pca = decomposition.PCA(n_components=150)\n",
    "#x_1hot_df = pca.fit_transform(x_1hot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_1hot_df, ys, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "Xb=pd.DataFrame(x_train)\n",
    "Yb=pd.DataFrame(y_train)\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "sm = KMeansSMOTE(k_neighbors=100, kmeans_estimator=1)\n",
    "\n",
    "x_train, y_train = sm.fit_resample(Xb, np.ravel(Yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Parameter tuning \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = linear_model.LogisticRegression(max_iter=10000)\n",
    "model = ensemble.RandomForestClassifier()\n",
    "\n",
    "model = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "model.fit(x_train, y_train)\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score: 0.805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_valid)  # predictions\n",
    "\n",
    "CV_accuracy = cross_val_score(model, x_train, y_train, scoring=\"balanced_accuracy\", cv=10)  # scoring\n",
    "print(f\"Balanced accuracy score: {np.mean(CV_accuracy):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved.\n",
      "  adding: mimic_synthetic_test.csv (deflated 91%)\n",
      "The submission is ready: submission_Mon_May_24_17-14-49_2021.zip\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test_1hot_df)  # compute predictions\n",
    "# VARS: x_test_df, x_test_fact_df, x_test_1hot_df, x_test_1hot_pcs\n",
    "# Models: model, search\n",
    "\n",
    "\n",
    "predictions_file = \"mimic_synthetic_test.csv\"\n",
    "\n",
    "pd.Series(predictions).to_csv(predictions_file, index=False, header=False)\n",
    "\n",
    "print(\"Predictions saved.\")\n",
    "\n",
    "t_stamp = time.asctime().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "\n",
    "output_file = f\"submission_{t_stamp}.zip\"\n",
    "\n",
    "!zip test_submission.zip mimic_synthetic_test.csv  # create a ZIP\n",
    "\n",
    "with ZipFile(output_file, \"w\") as z:\n",
    "    z.write(predictions_file)\n",
    "\n",
    "print(f\"The submission is ready: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
