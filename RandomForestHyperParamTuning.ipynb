{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imblearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (\n",
    "    decomposition,\n",
    "    discriminant_analysis,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    naive_bayes,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"public_data\")\n",
    "\n",
    "DROP_VARS = [\"ADMITTIME\", \"DISCHTIME\", \"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "features = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_feat.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_label.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "x_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_train.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_df.drop(columns=DROP_VARS, inplace=True)\n",
    "\n",
    "ys = pd.Series(\n",
    "    pd.read_csv(\n",
    "        DATA / \"mimic_synthetic_train.solution\",\n",
    "        header=None,\n",
    "        names=labels,\n",
    "        sep=\" \",\n",
    "    ).values.flatten()\n",
    ")\n",
    "\n",
    "# Load testing set\n",
    "x_test_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_test.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_test_df.drop(columns=DROP_VARS, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prev (train, test):\n",
    "    \"\"\"\n",
    "    Filling the cell containing NaN values with previous entry\n",
    "    \"\"\"\n",
    "    \n",
    "    na_cols = set(train.columns[train.isna().any()])\n",
    "    for col in na_cols:\n",
    "        train[col] = train[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    na_cols =  set(test.columns[test.isna().any()])\n",
    "    for col in na_cols:\n",
    "        test[col] = test[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, x_test_df = fill_prev(x_df, x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rare_categories(dfs, col, keeps=None, keep_n=5):\n",
    "    if keeps is None:\n",
    "        keeps = x_df[col].value_counts()[:keep_n].index\n",
    "        print(keeps)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.loc[~df[col].isin(keeps), col] = \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CATHOLIC', 'NOT_SPECIFIED', 'UNOBTAINABLE', 'PROTESTANT_QUAKER',\n",
      "       'JEWISH'],\n",
      "      dtype='object')\n",
      "Index(['ENGL', 'SPAN'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "merge_rare_categories([x_df, x_test_df], col=\"RELIGION\", keep_n=5)\n",
    "merge_rare_categories([x_df, x_test_df], col=\"LANGUAGE\", keep_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Hot enconding\n",
    "x_all_1hot_df = pd.get_dummies(pd.concat([x_df, x_test_df]))\n",
    "\n",
    "x_1hot_df = x_all_1hot_df[: len(x_df)]\n",
    "x_test_1hot_df = x_all_1hot_df[len(x_df) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobo/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "const_cols = {col for col in x_1hot_df if len(x_1hot_df[col].unique()) == 1}\n",
    "x_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n",
    "x_test_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_1hot_df = scaler.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df = scaler.fit_transform(x_test_1hot_df)\n",
    "\n",
    "pca = decomposition.PCA(n_components=150)\n",
    "x_1hot_df = pca.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df = pca.fit_transform(x_test_1hot_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_1hot_df, ys, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "Xb=pd.DataFrame(x_train)\n",
    "Yb=pd.DataFrame(y_train)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "x_train, y_train = sm.fit_resample(Xb, np.ravel(Yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation balanced accuracy: \n",
      "[0.79048253 0.79137063]\n",
      "The average score of cross validation:  0.790926583777383\n",
      " Balanced accuracy score: 0.7487729399880051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "results = cross_validate(lda, x_train, y_train, cv=2, scoring='balanced_accuracy')\n",
    "sorted(results.keys())\n",
    "\n",
    "\n",
    "print(\"Cross validation balanced accuracy: \")\n",
    "print(results['test_score'])\n",
    "print(\"The average score of cross validation: \", results['test_score'].mean())\n",
    "\n",
    "\n",
    "y_pred = lda.predict(x_valid)  # predictions\n",
    "score = balanced_accuracy_score(y_valid, y_pred)  # scoring\n",
    "print(\" Balanced accuracy score: {}\".format(score))\n",
    "\n",
    "\n",
    "#model = linear_model.LogisticRegression(max_iter=10000)\n",
    "#model = ensemble.RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test_1hot_df)  # compute predictions\n",
    "# VARS: x_test_df, x_test_fact_df, x_test_1hot_df, x_test_1hot_pcs\n",
    "# Models: model, search\n",
    "\n",
    "\n",
    "predictions_file = \"mimic_synthetic_test.csv\"\n",
    "\n",
    "pd.Series(predictions).to_csv(predictions_file, index=False, header=False)\n",
    "\n",
    "print(\"Predictions saved.\")\n",
    "\n",
    "t_stamp = time.asctime().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "\n",
    "output_file = f\"submission_{t_stamp}.zip\"\n",
    "\n",
    "!zip test_submission.zip mimic_synthetic_test.csv  # create a ZIP\n",
    "\n",
    "with ZipFile(output_file, \"w\") as z:\n",
    "    z.write(predictions_file)\n",
    "\n",
    "print(f\"The submission is ready: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
