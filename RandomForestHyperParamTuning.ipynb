{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imblearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (\n",
    "    decomposition,\n",
    "    discriminant_analysis,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    naive_bayes,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"public_data\")\n",
    "\n",
    "DROP_VARS = [\"ADMITTIME\", \"DISCHTIME\", \"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "features = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_feat.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_label.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "x_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_train.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_df.drop(columns=DROP_VARS, inplace=True)\n",
    "\n",
    "ys = pd.Series(\n",
    "    pd.read_csv(\n",
    "        DATA / \"mimic_synthetic_train.solution\",\n",
    "        header=None,\n",
    "        names=labels,\n",
    "        sep=\" \",\n",
    "    ).values.flatten()\n",
    ")\n",
    "\n",
    "# Load testing set\n",
    "x_test_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_test.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_test_df.drop(columns=DROP_VARS, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prev (train, test):\n",
    "    \"\"\"\n",
    "    Filling the cell containing NaN values with previous entry\n",
    "    \"\"\"\n",
    "    \n",
    "    na_cols = set(train.columns[train.isna().any()])\n",
    "    for col in na_cols:\n",
    "        train[col] = train[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    na_cols =  set(test.columns[test.isna().any()])\n",
    "    for col in na_cols:\n",
    "        test[col] = test[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, x_test_df = fill_prev(x_df, x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rare_categories(dfs, col, keeps=None, keep_n=5):\n",
    "    if keeps is None:\n",
    "        keeps = x_df[col].value_counts()[:keep_n].index\n",
    "        print(keeps)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.loc[~df[col].isin(keeps), col] = \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CATHOLIC', 'NOT_SPECIFIED', 'UNOBTAINABLE', 'PROTESTANT_QUAKER',\n",
      "       'JEWISH'],\n",
      "      dtype='object')\n",
      "Index(['ENGL', 'SPAN'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "merge_rare_categories([x_df, x_test_df], col=\"RELIGION\", keep_n=5)\n",
    "merge_rare_categories([x_df, x_test_df], col=\"LANGUAGE\", keep_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Hot enconding\n",
    "x_all_1hot_df = pd.get_dummies(pd.concat([x_df, x_test_df]))\n",
    "\n",
    "x_1hot_df = x_all_1hot_df[: len(x_df)]\n",
    "x_test_1hot_df = x_all_1hot_df[len(x_df) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "const_cols = {col for col in x_1hot_df if len(x_1hot_df[col].unique()) == 1}\n",
    "x_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n",
    "x_test_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_1hot_df = scaler.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df = scaler.fit_transform(x_test_1hot_df)\n",
    "\n",
    "#pca = decomposition.PCA(n_components=150)\n",
    "#x_1hot_df = pca.fit_transform(x_1hot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_1hot_df, ys, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:887: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "Xb=pd.DataFrame(x_train)\n",
    "Yb=pd.DataFrame(y_train)\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "sm = KMeansSMOTE(k_neighbors=100, kmeans_estimator=1)\n",
    "\n",
    "x_train, y_train = sm.fit_resample(Xb, np.ravel(Yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Parameter tuning \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 50, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "#model = linear_model.LogisticRegression(max_iter=10000)\n",
    "model = ensemble.RandomForestClassifier()\n",
    "\n",
    "model = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "model.fit(x_train, y_train)\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_valid)  # predictions\n",
    "\n",
    "CV_accuracy = cross_val_score(model, x_train, y_train, scoring=\"balanced_accuracy\", cv=10)  # scoring\n",
    "print(f\"Balanced accuracy score: {np.mean(CV_accuracy):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test_1hot_df)  # compute predictions\n",
    "# VARS: x_test_df, x_test_fact_df, x_test_1hot_df, x_test_1hot_pcs\n",
    "# Models: model, search\n",
    "\n",
    "\n",
    "predictions_file = \"mimic_synthetic_test.csv\"\n",
    "\n",
    "pd.Series(predictions).to_csv(predictions_file, index=False, header=False)\n",
    "\n",
    "print(\"Predictions saved.\")\n",
    "\n",
    "t_stamp = time.asctime().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "\n",
    "output_file = f\"submission_{t_stamp}.zip\"\n",
    "\n",
    "!zip test_submission.zip mimic_synthetic_test.csv  # create a ZIP\n",
    "\n",
    "with ZipFile(output_file, \"w\") as z:\n",
    "    z.write(predictions_file)\n",
    "\n",
    "print(f\"The submission is ready: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
