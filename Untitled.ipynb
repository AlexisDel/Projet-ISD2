{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imblearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (\n",
    "    decomposition,\n",
    "    discriminant_analysis,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    naive_bayes,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"public_data\")\n",
    "\n",
    "DROP_VARS = [\"ADMITTIME\", \"DISCHTIME\", \"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "features = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_feat.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_label.name\", header=None\n",
    ").values.flatten()\n",
    "\n",
    "x_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_train.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_df.drop(columns=DROP_VARS, inplace=True)\n",
    "\n",
    "ys = pd.Series(\n",
    "    pd.read_csv(\n",
    "        DATA / \"mimic_synthetic_train.solution\",\n",
    "        header=None,\n",
    "        names=labels,\n",
    "        sep=\" \",\n",
    "    ).values.flatten()\n",
    ")\n",
    "\n",
    "# Load testing set\n",
    "x_test_df = pd.read_csv(\n",
    "    DATA / \"mimic_synthetic_test.data\",\n",
    "    header=None,\n",
    "    names=features,\n",
    "    sep=\" \",\n",
    ")\n",
    "\n",
    "# Remove time related data that are not needed\n",
    "x_test_df.drop(columns=DROP_VARS, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prev (train, test):\n",
    "    \"\"\"\n",
    "    Filling the cell containing NaN values with previous entry\n",
    "    \"\"\"\n",
    "    \n",
    "    na_cols = set(train.columns[train.isna().any()])\n",
    "    for col in na_cols:\n",
    "        train[col] = train[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    na_cols =  set(test.columns[test.isna().any()])\n",
    "    for col in na_cols:\n",
    "        test[col] = test[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, x_test_df = fill_prev(x_df, x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rare_categories(dfs, col, keeps=None, keep_n=5):\n",
    "    if keeps is None:\n",
    "        keeps = x_df[col].value_counts()[:keep_n].index\n",
    "        print(keeps)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.loc[~df[col].isin(keeps), col] = \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CATHOLIC', 'NOT_SPECIFIED', 'UNOBTAINABLE', 'PROTESTANT_QUAKER',\n",
      "       'JEWISH'],\n",
      "      dtype='object')\n",
      "Index(['ENGL', 'SPAN'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "merge_rare_categories([x_df, x_test_df], col=\"RELIGION\", keep_n=5)\n",
    "merge_rare_categories([x_df, x_test_df], col=\"LANGUAGE\", keep_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Hot enconding\n",
    "x_all_1hot_df = pd.get_dummies(pd.concat([x_df, x_test_df]))\n",
    "\n",
    "x_1hot_df = x_all_1hot_df[: len(x_df)]\n",
    "x_test_1hot_df = x_all_1hot_df[len(x_df) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "const_cols = {col for col in x_1hot_df if len(x_1hot_df[col].unique()) == 1}\n",
    "x_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n",
    "x_test_1hot_df.drop(const_cols, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "pca = decomposition.PCA(n_components=150)\n",
    "x_1hot_df = pca.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df = pca.fit_transform(x_test_1hot_df)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_1hot_df = scaler.fit_transform(x_1hot_df)\n",
    "x_test_1hot_df = scaler.fit_transform(x_test_1hot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_1hot_df, ys, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:887: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "Xb=pd.DataFrame(x_train)\n",
    "Yb=pd.DataFrame(y_train)\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "sm = KMeansSMOTE(random_state=42, kmeans_estimator=1)\n",
    "x_train, y_train = sm.fit_resample(Xb, np.ravel(Yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = linear_model.LogisticRegression(max_iter=10000)\n",
    "model = ensemble.RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score: 0.988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_valid)  # predictions\n",
    "\n",
    "CV_accuracy = cross_val_score(model, x_train, y_train, scoring=\"balanced_accuracy\", cv=10)  # scoring\n",
    "print(f\"Balanced accuracy score: {np.mean(CV_accuracy):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved.\n",
      "\tzip warning: name not matched: #\n",
      "\tzip warning: name not matched: create\n",
      "\tzip warning: name not matched: a\n",
      "\tzip warning: name not matched: ZIP\n",
      "updating: mimic_synthetic_test.csv (164 bytes security) (deflated 99%)\n",
      "The submission is ready: submission_Mon_May_24_19-37-59_2021.zip\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test_1hot_df)  # compute predictions\n",
    "# VARS: x_test_df, x_test_fact_df, x_test_1hot_df, x_test_1hot_pcs\n",
    "# Models: model, search\n",
    "\n",
    "\n",
    "predictions_file = \"mimic_synthetic_test.csv\"\n",
    "\n",
    "pd.Series(predictions).to_csv(predictions_file, index=False, header=False)\n",
    "\n",
    "print(\"Predictions saved.\")\n",
    "\n",
    "t_stamp = time.asctime().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "\n",
    "output_file = f\"submission_{t_stamp}.zip\"\n",
    "\n",
    "!zip test_submission.zip mimic_synthetic_test.csv  # create a ZIP\n",
    "\n",
    "with ZipFile(output_file, \"w\") as z:\n",
    "    z.write(predictions_file)\n",
    "\n",
    "print(f\"The submission is ready: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
